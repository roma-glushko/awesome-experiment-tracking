# awesome-experiment-tracking
Curated list of available experiment tracking frameworks 

- [Aim](https://aimstack.io/) - logs your training runs, enables a beautiful UI to compare them and an API to query them programmatically
- [Sacred](https://github.com/IDSIA/sacred/) - configure, organize, log and reproduce experiments
- [Kedro](https://github.com/kedro-org/kedro) - open-source Python framework for creating reproducible, maintainable and modular data science code based on software engineering principles like modularity, separation of concerns and versioning
- [ClearML](https://github.com/allegroai/clearml) - a ML/DL development and production suite, it contains 4 main modules: Experiment Manager, MLOps, Data Management, Model Serving.
- [Polyaxon](https://polyaxon.com/) + [TraceML](https://github.com/polyaxon/traceml) - MLOps Tools For Managing & Orchestrating The Machine Learning Lifecycle
- [Keepsake](https://github.com/replicate/keepsake) - Version control for machine learning
- [MLFlow](https://github.com/mlflow/mlflow) - Open source platform for the machine learning lifecycle
- [GuildAI](https://guild.ai/) - brings systematic control to machine learning to help you build better models faster
- [TensorBoard](https://www.tensorflow.org/tensorboard/) - provides the visualization and tooling needed for machine learning experimentation

## Data Versioning

- [DVC](https://dvc.org/) - Version Control System for Machine Learning Projects

## SaaS

- [Weights & Biases](https://wandb.ai/site) - Build better models faster with experiment tracking, dataset versioning, and model management
- [Neptune.ai](https://neptune.ai/) - Experiment tracking and model registry for production teams 
- [Comet ML](https://www.comet.ml/site/) - Manage and optimize the entire ML lifecycle, from experiment tracking to model production monitoring
